{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1e2de6",
   "metadata": {},
   "source": [
    "# Assignment 1: MA338"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef98b4",
   "metadata": {},
   "source": [
    "#### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3076872",
   "metadata": {},
   "source": [
    "1. This Notebook will provide you the Tasks you have to complete, include a couple of questions, and define the necessary functions that you could use.\n",
    "2. You have to submit your modified version of this Notebook to FASER. Write in the title **Assignment-1-RegNumb-#YOUR REGISTRATION NUMBER#.ipynb**\n",
    "3. You can include as many extra cells as you need to make some comments (remember selecting Markdown cell instead of Code cell)\n",
    "4. Adding extra (related) stuff could help with grade: For example some nice plots/drawing (you can insert images on Jupyter notebooks, but you need to attach the image in the submission).\n",
    "5. This notebook will have several sections associated to different tasks you have to complete. In each one of them I will provide what functions you need to complete (for example policy evaluation, value iteration...)\n",
    "6. If you prefer not to follow the suggested structure but decide to program it in a different way, you can do it, as long as you explain all the processes and complete all the tasks.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b0119",
   "metadata": {},
   "source": [
    "#### General libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f9295f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.230524Z",
     "start_time": "2024-02-11T17:33:07.116702Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np # to work with mathematical operations\n",
    "import time # to reduce the speed of simulations\n",
    "import random # to randomly generate data\n",
    "import matplotlib.pyplot as plt # to plot\n",
    "from IPython.display import clear_output # Used to clearthe ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301d581",
   "metadata": {},
   "source": [
    "Run these commands to download and install the latest version of GridWorld (if you already installed it for the Lab 2, it should be ok)\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/andremht/gym-gridworld.git\n",
    "cd gym-gridworld\n",
    "pip3 install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1efa4346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:38:34.737391Z",
     "start_time": "2024-02-11T17:38:34.705133Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym_gridworld'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-24f250aa3d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym_gridworld\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym_gridworld'"
     ]
    }
   ],
   "source": [
    "import gym_gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6b40f",
   "metadata": {},
   "source": [
    "##### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22e8a1",
   "metadata": {},
   "source": [
    "The following functions can be useful to run the agents that you will create, as well as display the results in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc44bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:38:38.521031Z",
     "start_time": "2024-02-11T17:38:38.505031Z"
    }
   },
   "outputs": [],
   "source": [
    "## The following function will be use to evaluate the different policies.\n",
    "# The environment env will define what type of grid we will use\n",
    "# Agent will be defined according different policies\n",
    "# tsleep is used to create slower iterations so we can see how the agent moves in the grid\n",
    "\n",
    "\n",
    "def run_agent(env, agent, tsleep = 0.05):\n",
    "    state = env.reset()\n",
    "    time_step = 0\n",
    "    total_reward = 0\n",
    "    reward = 0;\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, done);\n",
    "        state, reward, done, info = env.step(action)  # step is defined into the environment, and it provides the new state, the reward obtained, and whether we finished or not\n",
    "        total_reward += reward\n",
    "        time_step += 1\n",
    "        clear_output(wait=True)  # to have a different displayed result on the screen\n",
    "        env.render()    # this code displays the agent state and action\n",
    "        print(\"Time step:\", time_step)\n",
    "        print(\"State:\", state)\n",
    "        print(\"Action:\", action)\n",
    "        print(\"Total reward:\", total_reward)\n",
    "        time.sleep(tsleep)  # to delay the transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f726374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:38:39.167827Z",
     "start_time": "2024-02-11T17:38:39.152078Z"
    }
   },
   "outputs": [],
   "source": [
    "#This function will an array and turn it into the grid format, and then plot the value function with a colourmap \n",
    "\n",
    "def plot_values(VF, size= (5,5), name = None):\n",
    "# reshape value function according to the size of the grid (rows and columns)\n",
    "    VF_grid = np.reshape(VF, size)\n",
    "\n",
    "# plot the state-value function\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(VF_grid, cmap='cool')\n",
    "    fig.colorbar(im, ax=ax)   #  colourbar to indicate which is higher and lower\n",
    "    for (j,i),label in np.ndenumerate(VF_grid):\n",
    "        ax.text(i, j, np.round(label, 5), ha='center', va='center', fontsize=14)\n",
    "    plt.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    plt.title(name+'-Value Function')\n",
    "    if name == 'Policy':\n",
    "            plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111817d",
   "metadata": {},
   "source": [
    "We can try some examples for the function plot_values, so you can have an idea what it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25c5cec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:38:42.710010Z",
     "start_time": "2024-02-11T17:38:42.676542Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = [1,3,4,6,20,4,1,5]\n",
    "Q = np.array(Q)\n",
    "V= 2*np.round(np.random.random_sample((16,)),2)+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffad37a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:38:44.740540Z",
     "start_time": "2024-02-11T17:38:44.191023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_values(V,size=(4,4), name='State')\n",
    "plot_values(Q, size=(4,2), name='Action')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5196679",
   "metadata": {},
   "source": [
    "***\n",
    "### Key variables in RL and OpenAI Gym\n",
    "\n",
    "For this assignment we will explore two variants of the Gridworld. The first one `'GridWorld-3x3-Wall-v0'` is a grid of 3x3 and contains a wall in some cells that do not allow to move on those directions. The second one is known as `FrozenLake-v1` which contains a few cells with holes that once the agent step on it, it drowns and the episode finishes. We will give more details once we start working with them. In the meantime a few reminders on OpenAI Gym variables.\n",
    "\n",
    "* __Agent__: The learner and decision maker. This is a class you should create.\n",
    "* __Environment__:  What the agent interacts with. This is the variable `env` in `env = gym.make(version of gridworld)`. In GridWorld, a particularly important variable is `env.P`, which contains the Markov process model of the system. This variable encodes the state transition probabilities, rewards, and other information. \n",
    "* __State__: A state $s \\in \\mathcal{S}$ is a succinct representation of the environments current state. The current sate can be obtained by `env.s`\n",
    "* __Action__: The agent can take actions $a \\in \\mathcal{A}$ in order to change the state of the environment. It is an element of `env.action_space`.\n",
    "* __Policy__: Rules for how the agent choses the next action given the current state, $a = \\pi(s)$.\n",
    "* __Reward__: An immediate reward $R(s,a)$ that the agent gets for taking action $a$ in state $s$. A reward depends on a state and action, so it can only be obtain through the `step` function, which in turn updates the environments state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d649e45",
   "metadata": {},
   "source": [
    "##  Dynamic Programming\n",
    "\n",
    "#### GridWorld 3x3 with a Wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64ec1bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:33:10.406731Z",
     "start_time": "2024-02-11T18:33:10.386697Z"
    }
   },
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment GridWorld-3x3-Wall doesn't exist. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-42765b084109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv_GW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GridWorld-3x3-Wall-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_GW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#will set the agent into a random initial state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Initial state:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reminder: python starts counting from 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"State space:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_GW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# observations and states will be the same for us in this module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Action space:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_GW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# all the available actions and their type: discrete or continuous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspec_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[0m_check_version_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No registered env with id: {id}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[1;34m(ns, name, version)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0m_check_name_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[1;34m(ns, name)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[0msuggestion_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Did you mean: `{suggestion[0]}`?\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     raise error.NameNotFound(\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[1;34mf\"Environment {name} doesn't exist{namespace_msg}. {suggestion_msg}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     )\n",
      "\u001b[1;31mNameNotFound\u001b[0m: Environment GridWorld-3x3-Wall doesn't exist. "
     ]
    }
   ],
   "source": [
    "env_GW = gym.make('GridWorld-3x3-Wall-v0')\n",
    "state = env_GW.reset()  #will set the agent into a random initial state\n",
    "print('Initial state:', state) #reminder: python starts counting from 0\n",
    "print(\"State space:\", env_GW.observation_space) # observations and states will be the same for us in this module\n",
    "print(\"Action space:\", env_GW.action_space) # all the available actions and their type: discrete or continuous\n",
    "env_GW.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303e3b0",
   "metadata": {},
   "source": [
    "__State space__: We see that the state space contains 9 discrete states. In this case each state corresponds to a position of the agent (3x3=9 possibilities).  \n",
    "But some of those states are not accesible, since there is a wall on them (states *4* and *7*.\n",
    "State *6* is the starting point **S**, and state *8* is the goal **G**. \n",
    "\n",
    "__Action space__: The 4 discrete actions corresponds to: 0 - Left, 1 - Down, 2 - Right, 3 - Up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9674fb11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:33:11.202448Z",
     "start_time": "2024-02-11T18:33:11.186674Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_GW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7c35b2404473>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_GW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Take action 3 (Up)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menv_GW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#displays the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"New state:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reward:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env_GW' is not defined"
     ]
    }
   ],
   "source": [
    "new_state, reward, done, info = env_GW.step(2) # Take action 3 (Up)\n",
    "env_GW.render() #displays the results\n",
    "print(\"New state:\", new_state)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259e7d6",
   "metadata": {},
   "source": [
    "We define now a uniform random agent (that does not learn, only acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df2bddf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:33:11.608957Z",
     "start_time": "2024-02-11T18:33:11.590915Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, nA, nS):\n",
    "        self.nA = nA\n",
    "        self.nS = nS\n",
    "        self.probs = np.ones((self.nS,self.nA))/self.nA #uniformly random probability (0.25 to each option)\n",
    "        \n",
    "\n",
    "    def act(self, state, done):\n",
    "        # IMPORTANT: the following code allows to select randomly an action, given a probability distribution p\n",
    "        action = np.random.choice(np.arange(self.nA), p=self.probs[state], replace = False) \n",
    "        return action # a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9569b696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:33:12.139483Z",
     "start_time": "2024-02-11T18:33:12.114571Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_GW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8c1bf2c7172a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent_uniform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_GW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_GW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#an instantiation of an agent of the class RandomAgent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'env_GW' is not defined"
     ]
    }
   ],
   "source": [
    "agent_uniform = RandomAgent(env_GW.nA, env_GW.nS) #an instantiation of an agent of the class RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "570e48cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:33:12.703776Z",
     "start_time": "2024-02-11T18:33:12.674521Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_GW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1aef0b3ef49f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_GW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_uniform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsleep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'env_GW' is not defined"
     ]
    }
   ],
   "source": [
    "run_agent(env_GW, agent_uniform, tsleep = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b9587",
   "metadata": {},
   "source": [
    "*Remark*: notice that in this grid we can easily spot the optimal policy, but a random agent struggles to find it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884fadd",
   "metadata": {},
   "source": [
    "### Task 1.1: Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b9caf",
   "metadata": {},
   "source": [
    "The following functions will help you to write the Bellman Equation for $v_{\\pi}$ in a iterative way. \n",
    "\n",
    "**Your task here is to write down the respective pseudo code for each function, or alternatively, the equations or backup diagrams associated. You can either write it directly here using LaTeX, or include an image in your submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f45a80b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T19:34:17.986875Z",
     "start_time": "2024-02-11T19:34:17.972834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function Q_from_V(environment, state, V)\n",
    "    {\n",
    "        action_values:= 1 x action_number vector of zeros\n",
    "        \n",
    "        for each action A do\n",
    "        \n",
    "            {\n",
    "                action_values[A] := compute the summation of p*(reward+ GAMMA * V[state'])\n",
    "            }\n",
    "        \n",
    "        return action_values\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "Function weighted_action_value(environment, agent, action_values, state)\n",
    "    {\n",
    "        for each action A do\n",
    "            {\n",
    "                result:= compute the summation of P(state|A) * value of the action\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "    }\n",
    "    \n",
    "\n",
    "Function Bellman_equation_RHS(environment, agent, old_V)\n",
    "    {\n",
    "        V:= 1 x number_of_states zero vector\n",
    "        \n",
    "        for each state s \n",
    "            {\n",
    "                if we are at terminal node then V[s] := 0\n",
    "                \n",
    "                otherwise\n",
    "            \n",
    "                    action_values := Q_from_V(environment, s, old_V)\n",
    "                    V[s] := weighted_action_value(environment, agent, action_values, s)\n",
    "    \n",
    "            }\n",
    "       \n",
    "        \n",
    "        return V\n",
    "        \n",
    "        \n",
    "    }\n",
    "\n",
    "'''\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4730adb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T01:07:24.728960Z",
     "start_time": "2024-02-13T01:07:24.690889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the discount factor\n",
    "GAMMA = 0.7 # defined as a global variable, so you can easily change it if needed.\n",
    "\n",
    "#For each action, computes the effect of the action, and update discounted reward for that action\n",
    "def Q_from_V(env, s, value_function):\n",
    "    action_values = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        for p, next_s, reward, _ in env.P[s][a]:\n",
    "            action_values[a] += p * (reward + GAMMA * value_function[next_s])   # a += 1 is the same than a = a+1\n",
    "    return action_values\n",
    "\n",
    "# Compute the expected state-value according to the action policy, \n",
    "# where the weights are the associated probabilities agent.probs\n",
    "def weighted_action_value(env, agent, action_values, s):\n",
    "    weighted_value = 0\n",
    "    action_set = range(agent.nA)\n",
    "    for action in action_set:\n",
    "        weighted_value += agent.probs[s , action] * action_values[action] \n",
    "    return weighted_value\n",
    "\n",
    "def Bellman_equation_RHS(env, agent, old_value_function):    #the righthand side of the Bellman Equation for V\n",
    "    value_function = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        if env.is_terminal(s):\n",
    "            value_function[s] = 0.0\n",
    "        else:\n",
    "            action_values = Q_from_V(env, s, old_value_function)\n",
    "            value_function[s] = weighted_action_value(env, agent, action_values, s)\n",
    "    return value_function\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a080b",
   "metadata": {},
   "source": [
    "### Task 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6fc78",
   "metadata": {},
   "source": [
    "The following function will compute the Iterative Policy Evaluation algorithm. **Yor task is to complete the following function where it says `# YOUR CODE HERE`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e1bd46b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T01:07:26.981242Z",
     "start_time": "2024-02-13T01:07:26.953890Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(env, agent, old_value, attempts=20, tol=1e-6):  \n",
    "    # attempts is written in case we don't converge fast enough. Probably not an issue for this problem, \n",
    "    # but for others it may be relevant\n",
    "    # old_value - we store the previous value here\n",
    "    for i in range(attempts):\n",
    "        # YOUR CODE HERE (you need to define how new_value is computed. You can the functions from Task 1.1)\n",
    "        new_value = Bellman_equation_RHS(env, agent, old_value)\n",
    "        if np.max(np.abs(new_value-old_value)) < tol:\n",
    "            break\n",
    "        old_value = new_value\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c29c0df8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T01:07:27.712351Z",
     "start_time": "2024-02-13T01:07:27.558031Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_GW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4d28adf3f4ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolicy_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_GW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_uniform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_GW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'env_GW' is not defined"
     ]
    }
   ],
   "source": [
    "policy_evaluation(env_GW, agent_uniform, np.zeros(env_GW.nS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10518a48",
   "metadata": {},
   "source": [
    "### Task 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f434d69",
   "metadata": {},
   "source": [
    "The following function will compute a *Policy Improvement* over the current policy $\\pi$.\n",
    "\n",
    "**Your task is to write that function.** HINT: you can use the function `Q_from_V` from **Task 1.1** to create the new policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27136553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.575409Z",
     "start_time": "2024-02-11T17:33:07.195Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, agent, value_function):\n",
    "    new_probs = np.ones([env.nS, env.nA]) / env.nA\n",
    "    stable_policy = False\n",
    "    for s in range(env.nS):\n",
    "        # Choose the best action in a current state under current policy\n",
    "        current_action = np.argmax(new_probs[s])\n",
    "        # Look one step ahead and evaluate if current action is optimal\n",
    "        # We will try every possible action in a current state\n",
    "        action_value = Q_from_V(env, s, value_function)\n",
    "        # Select a better action\n",
    "        best_action = np.argmax(action_value)\n",
    "        # If action didn't change\n",
    "        if current_action != best_action:\n",
    "            stable_policy = True\n",
    "            # Greedy policy update\n",
    "            new_probs[s] = np.eye(env.nA)[best_action]\n",
    "    agent.probs = new_probs\n",
    "    return new_probs, stable_policy\n",
    "# this function does not return anything, it just modifies the probabilities of the agent according to the \n",
    "# improved policy. You can also create an alternative version where the new policy is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60676ce",
   "metadata": {},
   "source": [
    "### Task 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d660c",
   "metadata": {},
   "source": [
    "The following function should combined both functions from  **Task 1.2** and **Task 1.3** to create a Policy Iteration Algorithm.\n",
    "\n",
    "**Your task is to write that function**. It should return both the Optimal Policy and the Optimal State Value Function. \n",
    "\n",
    "You can do this in more than one cell. For example use the following function to compute the optimal state value $v$, use that to obtain the optimal action value  $q$, and with that define the optimal policy. But you also can do it in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178922d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.575409Z",
     "start_time": "2024-02-11T17:33:07.198Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env, agent, value_function, MaxIter=1000, tol=1e-6): \n",
    "    #run it until no changes (tol) or until we run out of time (MaxIter)\n",
    "    optimal_value_function = value_function   #initialisation\n",
    "    optimal_policy = np.ones([env.nS, env.nA]) / env.nA  #  initialisation\n",
    "    policy_stable = False\n",
    "    i = 0\n",
    "    while (not policy_stable) and (i<=MaxIter):\n",
    "        i += 1\n",
    "        # Run the policy evaluation\n",
    "        optimal_value_function = policy_evaluation(env, agent, optimal_value_function)\n",
    "        # Run the policy improvement algorithm\n",
    "        optimal_policy, stable_policy = policy_improvement(env, agent, optimal_value_function)\n",
    "    return optimal_policy, optimal_value_function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2dceac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.575409Z",
     "start_time": "2024-02-11T17:33:07.201Z"
    }
   },
   "outputs": [],
   "source": [
    "policy_iteration(env_GW, agent_uniform, np.zeros(env_GW.nS))\n",
    "run_agent(env_GW,agent_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900f02f",
   "metadata": {},
   "source": [
    "### Task 1.5\n",
    "Similar to the previous point the following function is the Value Iteration Algorithm\n",
    "\n",
    "**Your task is to create that function**. The procedure should be similar to **Task 1.4** but you merged a couple of steps (sweeps) into one.  You should provide the Optimal State Function and the respective Optimal Policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb51b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.575409Z",
     "start_time": "2024-02-11T17:33:07.207Z"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, agent, value_function, MaxIter=1000, tol=1e-6):\n",
    "    optimal_value_function = value_function   #initialisation\n",
    "    optimal_policy = np.ones([env.nS, env.nA]) / env.nA  #  initialisation\n",
    "    for _ in range(MaxIter):\n",
    "        for s in range(env.nS):\n",
    "            optimal_value_function = Bellman_equation_RHS(env, agent, value_function)\n",
    "            optimal_policy[s,:] = Q_from_V(env, s, value_function)\n",
    "        if np.max(np.abs(optimal_value_function-value_function)) < tol:\n",
    "            break\n",
    "        value_function = optimal_value_function\n",
    "    ind = np.argwhere(optimal_policy==np.amax(optimal_policy,1, keepdims=True))\n",
    "    x = list(map(tuple, ind))\n",
    "    optimal_p = []\n",
    "    for i in range(len(x)):\n",
    "        optimal_p.append(x[i][1])\n",
    "    return optimal_value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960c261",
   "metadata": {},
   "source": [
    "### Task 1.6 Testing Gridworld\n",
    "\n",
    "Now that you have created the functions is time to test them on the environment. \n",
    "\n",
    "**Your task is to try different values of GAMMA and compare both Value Iteration and Policy Iteration on it**\n",
    "\n",
    "Write as many comments as you consider necessary, include some plots (for example use the function `plot_values` to display the state and action value functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3f467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.575409Z",
     "start_time": "2024-02-11T17:33:07.210Z"
    }
   },
   "outputs": [],
   "source": [
    "env_GW = gym.make('GridWorld-3x3-Wall-v0')\n",
    "state = env_GW.reset()  #will set the agent into a random initial state\n",
    "print('Initial state:', state) #reminder: python starts counting from 0\n",
    "print(\"State space:\", env_GW.observation_space) # observations and states will be the same for us in this module\n",
    "print(\"Action space:\", env_GW.action_space) # all the available actions and their type: discrete or continuous\n",
    "env_GW.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfa2bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.575409Z",
     "start_time": "2024-02-11T17:33:07.212Z"
    }
   },
   "outputs": [],
   "source": [
    "agent1 = RandomAgent(env_GW.nA, env_GW.nS)\n",
    "run_agent(env_GW, agent1, tsleep = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa9626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.575409Z",
     "start_time": "2024-02-11T17:33:07.215Z"
    }
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE: Improve agent1 policy using POLICY iteration algorithm and run the improved agent again \n",
    "# (you can do many runs to observe the improvement step by step)\n",
    "agent1 = RandomAgent(env_GW.nA, env_GW.nS)\n",
    "policy_iteration(env_GW, agent1, np.ones(env_GW.nS))\n",
    "run_agent(env_GW, agent1, tsleep = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7bc1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.591030Z",
     "start_time": "2024-02-11T17:33:07.218Z"
    }
   },
   "outputs": [],
   "source": [
    "agent2 = RandomAgent(env_GW.nA, env_GW.nS)\n",
    "#YOUR CODE HERE: Improve agent2 policy using VALUE iteration algorithm and run the improved agent again \n",
    "# (you can do many runs to observe the improvement step by step)\n",
    "value_iteration(env_GW, agent1, np.ones(env_GW.nS))\n",
    "run_agent(env_GW, agent2, tsleep = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e266d",
   "metadata": {},
   "source": [
    "**Question** Can you measure which method find the optimal policy faster? (if any)\n",
    "\n",
    "**Ans:** It seems policy iteration method reflects better performance with this sample problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db1321",
   "metadata": {},
   "source": [
    "###  Frozen Lake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c49672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.591030Z",
     "start_time": "2024-02-11T17:33:07.224Z"
    }
   },
   "outputs": [],
   "source": [
    "env_FL = gym.make('FrozenLake-v1',is_slippery=False)\n",
    "state = env_FL.reset()  #will set the agent into a random initial state\n",
    "print('Initial state:', state) #reminder: python starts counting from 0\n",
    "print(\"State space:\", env_FL.observation_space) # observations and states will be the same for us in this module\n",
    "print(\"Action space:\", env_FL.action_space) # all the available actions and their type: discrete or continuous\n",
    "env_FL.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e0a77",
   "metadata": {},
   "source": [
    "`FrozenLake-v1` is an environment that simulates the following situation: imagine your agent wants to grab something on the other side of the Frozen Lake. Most of the lake is frozen, but there are a couple of cells with thin ice (holes) that will break if it steps on them. The objective is to arrive to the goal as quick as possible without falling into any holes.\n",
    "There are more than one terminal state: either reaches the goal or falls into the hole.  Every transition has a `reward=0` except when the agent reaches the goal, which has  `reward=1`.\n",
    "\n",
    "By default FrozenLake comes with the attribute `is_slippery = True` which means that even if the agent decides to go (for example) `right`, it might end up `up, left, down` to its current position (with certain probability). For simplicity we will call the environment with the parameter `is_slippery = False` (deterministic case), but you are welcome to explore the general case if you have time.\n",
    "\n",
    "1. **S**: Starting Point\n",
    "2. **G**: Goal\n",
    "3. **F**: Frozen (safe cell that the agent can step on it)\n",
    "4. **H**: Hole (thin ice that will break if the agent steps on it will die) \n",
    "\n",
    "As usual, actions are given by `left = 0, down = 1, right =1, up = 3`.\n",
    "\n",
    "The function `env_FL.P[s][a]` will provide the set of [probability, new_state, reward,done]. I say **set**, because in the *slippery* case, it provides all the different options. In the non-slippery case it will give a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce5370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.591030Z",
     "start_time": "2024-02-11T17:33:07.227Z"
    }
   },
   "outputs": [],
   "source": [
    "current_state = 10  \n",
    "action = 0  # Left \n",
    "#[(probability, new_state, reward, done)] = env_FL.P[current_state][action]\n",
    "print(env_FL.P[current_state][action])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3255a",
   "metadata": {},
   "source": [
    "Let us create a random agent and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f51d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.591030Z",
     "start_time": "2024-02-11T17:33:07.229Z"
    }
   },
   "outputs": [],
   "source": [
    "agent3 = RandomAgent(env_FL.nA, env_FL.nS)\n",
    "run_agent(env_FL, agent3, tsleep = 0.5) #if the agent dies too quickly, increase the value of tsleep to observe the transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2b00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.591030Z",
     "start_time": "2024-02-11T17:33:07.232Z"
    }
   },
   "outputs": [],
   "source": [
    "#agent3.probs[:,1]=.1\n",
    "#agent3.probs[2,3] =.8\n",
    "plot_values(agent3.probs, size=(env_FL.nS, env_FL.nA), name = 'Policy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1f4c4",
   "metadata": {},
   "source": [
    "### Task 1.7 Testing Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc23c87",
   "metadata": {},
   "source": [
    "Similar to the case of GridWorld, **your task improve the policy of Agent3 using both Policy Iteration and Value Iteration**.\n",
    "Play around with the parameters (GAMMA, MaxIter) to see if they have any effect on avoiding the death of your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b0837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.591030Z",
     "start_time": "2024-02-11T17:33:07.235Z"
    }
   },
   "outputs": [],
   "source": [
    "policy_iteration(env_GW, agent3, np.ones(env_GW.nS))\n",
    "run_agent(env_FL, agent3, tsleep=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e670b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:33:08.606651Z",
     "start_time": "2024-02-11T17:33:07.240Z"
    }
   },
   "outputs": [],
   "source": [
    "value_iteration(env_GW, agent3, np.ones(env_GW.nS))\n",
    "run_agent(env_FL, agent3, tsleep=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6afbf",
   "metadata": {},
   "source": [
    "Changing value of GAMA(from 0.02, 0.5, 0.2, 0.4, 0.7, 1.0) and maxIter does not affect the convergence of Frozen Lake game. It seems both method may need some correction to enable them to beat this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fabfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3bb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62f038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95e4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
